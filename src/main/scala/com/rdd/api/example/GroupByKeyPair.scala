package com.rdd.api.example

import org.apache.spark.SparkContext

object GroupByKeyPair {
  def main(args: Array[String]) {

    val sc = new SparkContext("local", "GroupByKeyPair Test")
    val d = sc.parallelize(1 to 100, 10)

    val pairs = d.keyBy(x => x % 10) // maps value of d to a key generated by x%10

    val result1 = pairs.groupByKey()
    //val result2 = pairs.groupByKey(3)
    //val result3 = pairs.groupByKey(new RangePartitioner(3, pairs))

    println("Result 1:")
    result1.foreach(println)

    //println("Result 2:")
    //result2.foreach(println)

    //println("Result 3:")
    //result3.foreach(println)

  }
}